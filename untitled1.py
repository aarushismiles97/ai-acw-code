# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ng5TDhDzmtGhE8if9VNOVNhfxzs4w7YL
"""

from google.colab import drive

drive.mount('/content/gdrive')

import pandas as pd

traindata = pd.read_csv('gdrive/My Drive/deforestation_sentiment_train.csv', usecols=["text","distillbert_valence"], na_values = ['no info', '.'])
testdata = pd.read_csv('gdrive/My Drive/deforestation_sentiment_val.csv', usecols=["text","distillbert_valence"], na_values = ['no info', '.'])

text= traindata['text'].values

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

def convert_decimals(decimals):
  converted_decimals = []
  for decimal in decimals:
    if decimal < 0:
      converted_decimals.append("negative")
    else:
      converted_decimals.append("positive")
  return converted_decimals

import re
def process_sentence(sentence):
  return re.sub(r'[\\\\/:*«`\'?¿";!<>,.|]', '', sentence.lower().strip())

X_train = traindata["text"].apply(process_sentence)
y_train= convert_decimals(traindata["distillbert_valence"])

X_test = testdata["text"].apply(process_sentence)
y_test= convert_decimals(testdata["distillbert_valence"])

labels= set(y_train)

elements = (' '.join([sentence for sentence in X_train])).split()
elements.append("<UNK>")

def create_lookup_tables(text):
  vocab = set(text)
  vocab_to_int = {word: i for i, word in enumerate(vocab)}
  int_to_vocab = {v:k for k, v in vocab_to_int.items()}
  return vocab_to_int, int_to_vocab

# Map our vocabulary to int
vocab_to_int, int_to_vocab = create_lookup_tables(elements)
labels_to_int, int_to_labels = create_lookup_tables(y_train)

print("Vocabulary of our dataset: {}".format(len(vocab_to_int)))

#One-Hot encoding
def convert_to_int(data, data_int):
  all_items = []
  for sentence in data:
    all_items.append([data_int[word] if word in data_int else data_int["<UNK>"] for word in sentence.split()])
  return all_items

# Convert our inputs
X_test_encoded = convert_to_int(X_test, vocab_to_int)
X_train_encoded = convert_to_int(X_train, vocab_to_int)

y_data = convert_to_int(y_test, labels_to_int)

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit(y_data)

y_train_encoded = enc.fit_transform(convert_to_int(y_train,labels_to_int)).toarray()
y_test_encoded = enc.fit_transform(convert_to_int(y_test, labels_to_int)).toarray()

print(y_train_encoded[:10],'\n', y_train[:10])

import tensorflow as tf
# Import Keras
from tensorflow import keras
from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Embedding
from keras.preprocessing import sequence
# Hyperparameters
max_sentence_length = 200
embedding_vector_length = 300
dropout = 0.5

# Truncate and pad input sentences
X_train_pad = keras.preprocessing.sequence.pad_sequences(X_train_encoded, maxlen=max_sentence_length)
X_test_pad = keras.preprocessing.sequence.pad_sequences(X_test_encoded, maxlen=max_sentence_length)

# Create the model
model = Sequential()
model.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))

model.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))
model.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))
model.add(Dense(len(labels), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
# Train the model
model.fit(X_train_pad, y_train_encoded, epochs=10, batch_size=256, validation_data=(X_test_pad, y_test_encoded))
# Final evaluation of the model
scores = model.evaluate(X_test_pad, y_test_encoded, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))







